{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T16:37:18.498541Z",
     "start_time": "2025-01-28T16:37:18.495068Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install tf-keras\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:11:12.585094Z",
     "start_time": "2025-01-28T17:11:05.863787Z"
    }
   },
   "outputs": [],
   "source": [
    "#input_file = \"maharashtra_data.csv\"\n",
    "#Docs = pd.read_csv(input_file)\n",
    "loader = PyPDFLoader(\"i3325e.pdf\")\n",
    "Docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:10:53.923398Z",
     "start_time": "2025-01-28T17:10:53.889003Z"
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap = 0)\n",
    "chunks = text_splitter.split_documents(Docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T16:54:36.750364Z",
     "start_time": "2025-01-28T16:54:36.744081Z"
    }
   },
   "outputs": [],
   "source": [
    "print(type(chunks))\n",
    "print(type(chunks[0]))\n",
    "text_chunk = [chunk.page_content for chunk in chunks]\n",
    "print(type(text_chunk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:00:22.501145Z",
     "start_time": "2025-01-28T16:58:36.532384Z"
    }
   },
   "outputs": [],
   "source": [
    "res = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = res.encode(text_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T17:00:27.048485Z",
     "start_time": "2025-01-28T17:00:27.039457Z"
    }
   },
   "outputs": [],
   "source": [
    "print(embeddings[600])\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T18:42:48.043402Z",
     "start_time": "2025-01-28T18:42:37.984036Z"
    }
   },
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path='database')\n",
    "collection  =client.get_or_create_collection('collection')\n",
    "\n",
    "ids = [f\"doc_{i}\" for i in range(len(embeddings))]\n",
    "\n",
    "# Define the maximum batch size\n",
    "max_batch_size = 166\n",
    "\n",
    "# Split the data into smaller batches\n",
    "for i in range(0, len(embeddings), max_batch_size):\n",
    "\tbatch_ids = ids[i:i + max_batch_size]\n",
    "\tbatch_documents = text_chunk[i:i + max_batch_size]\n",
    "\tbatch_embeddings = embeddings[i:i + max_batch_size]\n",
    "\tcollection.add(ids=batch_ids, documents=batch_documents, embeddings=batch_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.peek()\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[455])\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "model = Mistral()\n",
    "\n",
    "# load_dotenv()  # Load environment variables\n",
    "\n",
    "# api_key = os.getenv(\"D:/Programs/Nirman_2025/Enviroment.env\")\n",
    "# db_user = os.getenv(\"DB_USER\")\n",
    "# db_pass = os.getenv(\"DB_PASS\")\n",
    "\n",
    "# print(api_key, db_user, db_pass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Query = \"what type of soil is reqquired for crop\"\n",
    "results = collection.query(\n",
    "    query_texts= Query, # Chroma will embed this for you\n",
    "    n_results=20 # how many results to return\n",
    ")\n",
    "\n",
    "print(results)'''\n",
    "Query  =\"HOw to imporve soil fertility? \"\n",
    "query_embedding = res.encode([Query])\n",
    "results = collection.query(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from mistralai import Mistral\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain import VectorDBQA, PromptTemplate\n",
    "\n",
    "geneartor = Mistral()\n",
    "\n",
    "template = PromptTemplate(input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {Query}\")\n",
    "#qa_pipeline =  VectorDBQA(vector_store=collection, embeddings_model = res , prompt_template = template, language_model = geneartor)\n",
    "response = qa_pipeline(Query)\n",
    "\n",
    "print(response)'''\n",
    "import os\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# Set your Mistral AI API key (Replace \"your-api-key\" with your actual key)\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"236mWUjffs24Rg2pkQNfQiJNxg9EUxNO\"\n",
    "\n",
    "def generate_response(prompt: str, model_name: str = \"mistral-small\", temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response using the Mistral AI model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input question or prompt.\n",
    "        model_name (str): The Mistral model to use (default: \"mistral-small\").\n",
    "        temperature (float): The temperature setting for creativity (default: 0.7).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from Mistral AI.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Mistral AI model with API key\n",
    "        generator = ChatMistralAI(\n",
    "            model=model_name,  # Model name (e.g., \"mistral-small\")\n",
    "            temperature=temperature,\n",
    "            mistral_api_key=os.getenv(\"MISTRAL_API_KEY\")  # Fetch API key from env variable\n",
    "        )\n",
    "\n",
    "        # Generate and return the response\n",
    "        response = generator.predict(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     user_prompt = \"How to imporve fertility in soil\"\n",
    "#     response = generate_response(prompt=user_prompt)\n",
    "#     print(\"Generated Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Input prompt\n",
    "prompt = \"which crops to grow in low moisture and harsh winter season?\"\n",
    "\n",
    "# Call the function\n",
    "response = generate_response(prompt=prompt)\n",
    "\n",
    "# Display the response\n",
    "print(\"Generated Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which crops to grow in low moisture and harsh winter season\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hello' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhello\u001b[49m \n",
      "\u001b[1;31mNameError\u001b[0m: name 'hello' is not defined"
     ]
    }
   ],
   "source": [
    "hello "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hii im your ai assistant "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
